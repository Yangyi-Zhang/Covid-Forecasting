{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from matplotlib.pyplot import figure\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "# !pip install scikit-learn==0.24\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_lattice as tfl\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"CACases.csv\", index_col = False)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counties we are interested in\n",
    "counties = [\"Los Angeles\", \"San Diego\", \"San Francisco\", \"Santa Barbara\",\\\n",
    "            \"Fresno\", \"Sacramento\", \"Ventura\", \"Riverside\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time series of cases, each value is an average of 7 previous days(include the current date)\n",
    "# for each county\n",
    "df1 = df.copy()  # deep copy\n",
    "df1 = df1.iloc[6:]\n",
    "for county in counties:\n",
    "    # holder\n",
    "    temp = []\n",
    "    # take average, starting from 7th day\n",
    "    for i in range(6,len(df)):\n",
    "        # remove day of week effect\n",
    "        ave = np.mean(df[county].iloc[i-6:i+1])  # 6 previous days and the exact date, 7 days' average\n",
    "        temp.append(ave)\n",
    "    df1[county] = temp\n",
    "df1.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Smoothing\n",
    "- Remove day of week effect by replace $y_t$ with an average of 7 previous days(inclusive)\n",
    "    - Each value is an average of data from all 7 different weekdays, thud day of week effect removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time series of cases, each value is an average of 7 previous days(include the current date)\n",
    "# for each county\n",
    "df1 = df.copy()  # deep copy\n",
    "df1 = df1.iloc[6:]\n",
    "for county in counties:\n",
    "    # holder\n",
    "    temp = []\n",
    "    # take average, starting from 7th day\n",
    "    for i in range(6,len(df)):\n",
    "        # remove day of week effect\n",
    "        ave = np.mean(df[county].iloc[i-6:i+1])  # 6 previous days and the exact date, 7 days' average\n",
    "        temp.append(ave)\n",
    "    df1[county] = temp\n",
    "df1.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: original dataset, county we want\n",
    "# output: all data, sorted by date, of that county\n",
    "def extract(dataset, county):\n",
    "    # select data only from the input county\n",
    "    temp = dataset.to_dict()[county]\n",
    "    temp = pd.Series(temp)\n",
    "    # reset index\n",
    "    # temp.reset_index(drop=True, inplace=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = extract(df1, \"Los Angeles\")\n",
    "la_raw = extract(df, \"Los Angeles\")\n",
    "la_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_raw.plot(label = 'raw')\n",
    "la.plot(linewidth = 2, label = \"smoothed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Differencing\n",
    "- Differencing of time series in discrete time\n",
    "- transformation of series to a new time series wehre values are the difference between consecutive values of previous series\n",
    "- help stablizing the mean of time series by removing trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a differenced series, output as Series\n",
    "# interval: order of differencing, default = 1\n",
    "def difference(dataset, interval=1):\n",
    "    diff = []\n",
    "    for i in range(interval, len(dataset)):\n",
    "        # no need to difference\n",
    "        if (interval == 0):\n",
    "            value = dataset[i]\n",
    "        else:\n",
    "            value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "# invert differenced value\n",
    "# input: single original, single prediction, interval(default as 1)\n",
    "def invert_difference(history, yhat, interval=1):\n",
    "    if (interval == 0):\n",
    "        return yhat\n",
    "    else:\n",
    "        return yhat + history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "- Scale the data to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "# input: train(np array, train_size*1), test(np array, test_size*1)\n",
    "def scale(train, test):\n",
    "    # find scalers\n",
    "    aveTrain = np.mean(train)\n",
    "    maxTrain = max(train)\n",
    "    minTrain = min(train)\n",
    "    scaler = [aveTrain, maxTrain, minTrain]\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = (train - aveTrain)/(maxTrain-minTrain)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = (test - aveTrain)/(maxTrain-minTrain)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "# inverse scaling for a forecasted value\n",
    "# input: scaler, single prediction yhat\n",
    "def invert_scale(scaler, yhat):\n",
    "    inverted = yhat*(scaler[1]-scaler[2]) + scaler[0]\n",
    "    return inverted[0][0]  # output: a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = [], []\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg  # output: a data frame\n",
    "# prepare data for AR\n",
    "def prepare_AR(series, diff, train_size, test_size, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    raw_values = raw_values.reshape(len(raw_values), 1)\n",
    "    # differencing\n",
    "    diff_series = difference(raw_values, diff)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # split into train & test\n",
    "    train_diff, test_diff = diff_values[:(train_size-diff)], diff_values[(train_size-diff):]\n",
    "    # rescaling\n",
    "    scaler, train_scaled, test_scaled = scale(train_diff, test_diff)\n",
    "    # adjust data type for test_scaled\n",
    "    # test_scaled = [row[0][0] for row in test_scaled]\n",
    "    # convert to supervised\n",
    "    train = [row[0][0] for row in train_scaled]\n",
    "    test = series_to_supervised(test_scaled, n_lag, n_seq).values\n",
    "    return scaler, train, test  # scaler: list, train: list, test: np array\n",
    "# prepare data for neural networks\n",
    "def prepare_nn(series, diff, train_size, test_size, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    raw_values = raw_values.reshape(len(raw_values), 1)\n",
    "    # differencing\n",
    "    diff_series = difference(raw_values, diff)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # split into train & test\n",
    "    train_diff, test_diff = diff_values[:(train_size-diff)], diff_values[(train_size-diff):]\n",
    "    # rescaling\n",
    "    scaler, train_scaled, test_scaled = scale(train_diff, test_diff)\n",
    "    # adjust data type for train_scaled, test_scaled\n",
    "    train_scaled = [row[0][0] for row in train_scaled]\n",
    "    test_scaled = [row[0][0] for row in test_scaled]\n",
    "    # convert to supervised\n",
    "    train = series_to_supervised(train_scaled, n_lag, n_seq).values\n",
    "    test = series_to_supervised(test_scaled, n_lag, n_seq).values\n",
    "    return scaler, train, test  # scaler: list of np array, train: np array, test: np array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(layers.Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_layer(keras.layers.Layer):\n",
    "    def __init__(self, input_dim=1, units=1):\n",
    "        super(alpha_layer, self).__init__()\n",
    "        # check with initializer.get_config()\n",
    "        initializer = tf.keras.initializers.RandomUniform(minval = 0, maxval = 1)\n",
    "        self.alpha = self.add_weight(shape=(input_dim, units), initializer=initializer, \n",
    "                                     constraint=lambda x: tf.clip_by_value(x, 0, 1), trainable=True)\n",
    "\n",
    "    def call(self, input1, input2):\n",
    "        return tf.matmul(input1, self.alpha) + tf.matmul(input2, (1-self.alpha))\n",
    "# fit the hybrid model\n",
    "def fit_comb(train, n_lag, n_seq, n_batch, n_epoch, n_neurons):\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    Z = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # AR layer\n",
    "    input1 = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "    AR = tfl.layers.Linear(num_input_dims=X.shape[1], units=n_seq)(input1)\n",
    "    \n",
    "    # lstm layer\n",
    "    input2 = tf.keras.layers.Input(shape=(Z.shape[1],Z.shape[2]))\n",
    "    lstm = layers.LSTM(n_seq)(input2)\n",
    "    # fully connected lstm\n",
    "    lstm_connected = tf.keras.layers.Dense(n_seq)(lstm)\n",
    "    \n",
    "    # alpha layer\n",
    "    layer = alpha_layer()\n",
    "    #added = layer(AR, lstm)\n",
    "    added = layer(AR, lstm_connected)\n",
    "    model = tf.keras.models.Model(inputs=[input1,input2], outputs=added)\n",
    "    # compile before training or testing\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(n_epoch):\n",
    "        model.fit([X, Z], y, epochs=1, batch_size=n_batch, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def make_AR(model, raw_value, test, test_size, n_lag, n_seq, diff, scaler):\n",
    "    forecasts = []\n",
    "    prev = raw_value[-(test_size+diff):(len(raw_value)-diff)]  # Y_(t-diff)  # (train_size+n_lag-1):-1\n",
    "    for i in range(test_size):\n",
    "        X = test[i, 0:n_lag]  # X_test, will not use y_test\n",
    "        # make forecast\n",
    "        forecast = model.params[0]  # initlialize with coefficient\n",
    "        for j in range(n_lag):\n",
    "            forecast += model.params[j+1] * X[j]  # forecast = a0 + a_i * X_i for i = 1,...,7\n",
    "        # invert scaling\n",
    "        forecast = invert_scale(scaler, forecast)\n",
    "        # invert differencing\n",
    "        forecast = invert_difference(prev[i], forecast, diff)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]][0]\n",
    "# evaluate the model\n",
    "def make_lstm(model, n_batch, raw_value, test, test_size, n_lag, n_seq, diff, scaler):\n",
    "    forecasts = []\n",
    "    prev = raw_value[-(test_size+diff):(len(raw_value)-diff)]  # Y_(t-diff)\n",
    "    for i in range(len(test)):\n",
    "        X = test[i, 0:n_lag]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # invert scaling\n",
    "        forecast = invert_scale(scaler, forecast)\n",
    "        # invert differencing\n",
    "        forecast = invert_difference(prev[i], forecast)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with a hybird model\n",
    "def forecast_comb(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    Z = X.reshape(1, 1, len(X))\n",
    "    X = X.reshape(1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict([X,Z], batch_size=n_batch)\n",
    "    return [x for x in forecast[0, :]][0]\n",
    "    return [x for x in forecast[0, :]][0]\n",
    "# evaluate the model\n",
    "def make_comb(model, n_batch, raw_value, test, test_size, n_lag, n_seq, diff, scaler):\n",
    "    forecasts = []\n",
    "    prev = raw_value[-(test_size+diff):(len(raw_value)-diff)]  # Y_(t-diff)\n",
    "    for i in range(len(test)):\n",
    "        X = test[i, 0:n_lag]\n",
    "        # make forecast\n",
    "        forecast = forecast_comb(model, X, n_batch)\n",
    "        # invert scaling\n",
    "        forecast = invert_scale(scaler, forecast)\n",
    "        # invert differencing\n",
    "        forecast = invert_difference(prev[i], forecast)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with RMSE, MAE, MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate RMSE for each step in forcasting\n",
    "def evaluate_forecasts(truth, forecasts):\n",
    "    #rmse = np.sqrt(mean_squared_error(truth, forecasts))\n",
    "    #mae = mean_absolute_error(truth, forecasts)\n",
    "    truth = np.array(truth)\n",
    "    forecasts = np.array(forecasts)\n",
    "    mape = mean_absolute_percentage_error(truth, forecasts)*100\n",
    "    #return rmse, mae, mape\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty(data):\n",
    "    Mean = []\n",
    "    Std = []\n",
    "    for i in range(test_size):\n",
    "        temp = []\n",
    "        for j in range(len(data)):\n",
    "            temp.append(data[j][i])  # all predictions for the i's date\n",
    "        mean, std = mean_confidence_interval(temp)\n",
    "        Mean.append(mean)\n",
    "        Std.append(std)\n",
    "    return np.array(Mean), np.array(Std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform on Interesting Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given historical observations (t, t-1, t-2, â€¦ t-n+1) forecast t+1(future 1 day)\n",
    "n_lag = 7  # number of lag, use 7 past days\n",
    "n_seq = 1  # predict 1 future days\n",
    "diff = 1 # order of difference\n",
    "train_size = 63  # train set size\n",
    "test_size = 18 # test set size\n",
    "\n",
    "n_step = 7 \n",
    "\n",
    "n_batch = 1\n",
    "nb_epoch = 100\n",
    "n_neurons = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1\n",
    "## Curved Training Data and Down Trend Testing: San Diego 2020-12-03 to 2021-02-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"San Diego\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[300:300+(train_size+test_size+n_lag)]\n",
    "raw_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "plt.legend(fontsize= \"large\")\n",
    "plt.savefig(\"SD_raw_2020-12-03_2021-02-28.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_sd = []  # mape\n",
    "pred_sd = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_sd.append(temp)\n",
    "    pred_sd.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sd[i][0]) \n",
    "    lstm.append(pred_sd[i][1])\n",
    "    hybrid.append(pred_sd[i][2])\n",
    "    \n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"San Diego\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[300:300+(train_size+test_size+n_lag)]\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"SD_2020-12-03_2021-02-28.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mean mape\n",
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_sd[i][0]) \n",
    "    mape_lstm.append(mape_sd[i][1])\n",
    "    mape_hybrid.append(mape_sd[i][2])\n",
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check maximum standard error\n",
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sd[i][0]) \n",
    "    lstm.append(pred_sd[i][1])\n",
    "    hybrid.append(pred_sd[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(max(lstm_std))\n",
    "print(max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2\n",
    "## Up Trend Training and Down Trend Testing: San Francisco 2020-02-17 to 2020-05-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"San Francisco\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[10:10+(train_size+test_size+n_lag)]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "#plt.title(\"Training and Testing Set\", fontname = \"Times New Roman\", fontsize='large', fontweight='bold')\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.savefig(\"SF_raw_2020-02-17_2020-05-14.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_sf = []  # mape\n",
    "pred_sf = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_sf.append(temp)\n",
    "    pred_sf.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sf[i][0]) \n",
    "    lstm.append(pred_sf[i][1])\n",
    "    hybrid.append(pred_sf[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"San Francisco\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[10:10+(train_size+test_size+n_lag)]\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"SF_2020-02-17_2020-05-14.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_sf[i][0]) \n",
    "    mape_lstm.append(mape_sf[i][1])\n",
    "    mape_hybrid.append(mape_sf[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sf[i][0]) \n",
    "    lstm.append(pred_sf[i][1])\n",
    "    hybrid.append(pred_sf[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(max(lstm_std))\n",
    "print(max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3\n",
    "## Up Trend Training and Up Trend Testing: Los Angeles 2020-09-24 to 2020-12-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"Los Angeles\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[230:230+(train_size+test_size+n_lag)]\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "#plt.title(\"Training and Testing Set\", fontname = \"Times New Roman\", fontsize='large', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.savefig(\"LA_raw_2020-09-24_2020-12-20.png\", dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_la = []  # mape\n",
    "pred_la = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_la.append(temp)\n",
    "    pred_la.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_la[i][0]) \n",
    "    lstm.append(pred_la[i][1])\n",
    "    hybrid.append(pred_la[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"Los Angeles\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[230:230+(train_size+test_size+n_lag)]\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"LA_2020-09-24_2020-12-20.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_la[i][0]) \n",
    "    mape_lstm.append(mape_la[i][1])\n",
    "    mape_hybrid.append(mape_la[i][2])\n",
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(lstm_std))\n",
    "print(np.mean(hybrid_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_la[i][0]) \n",
    "    lstm.append(pred_la[i][1])\n",
    "    hybrid.append(pred_la[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(np.max(lstm_std))\n",
    "print(np.max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4\n",
    "## Down Trend Training and Down Trend Testing: San Francisco 2022-06-10 to 2022-09-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"San Francisco\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[-(train_size+test_size+n_lag):]  # last trial\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "#plt.title(\"Training and Testing Set\", fontname = \"Times New Roman\", fontsize='large', fontweight='bold')\n",
    "plt.legend(fontsize = \"large\")\n",
    "plt.savefig(\"SF_raw_2022-06-10_2022-09-05.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_sf_2 = []  # mape\n",
    "pred_sf_2 = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_sf_2.append(temp)\n",
    "    pred_sf_2.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sf_2[i][0]) \n",
    "    lstm.append(pred_sf_2[i][1])\n",
    "    hybrid.append(pred_sf_2[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"San Francisco\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[-(train_size+test_size+n_lag):]  # last trial\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"SF_2022-06-10_2022-09-05.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_sf_2[i][0]) \n",
    "    mape_lstm.append(mape_sf_2[i][1])\n",
    "    mape_hybrid.append(mape_sf_2[i][2])\n",
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sf_2[i][0]) \n",
    "    lstm.append(pred_sf_2[i][1])\n",
    "    hybrid.append(pred_sf_2[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(max(lstm_std))\n",
    "print(max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5\n",
    "## Down Trend Training and Up Trend Testing: Santa Barbara 2022-01-17 to 2022-04-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"Santa Barbara\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[710:710+(train_size+test_size+n_lag)]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "#plt.title(\"Training and Testing Set\", fontname = \"Times New Roman\", fontsize='large', fontweight='bold')\n",
    "plt.legend(fontsize = \"large\")\n",
    "# plt.savefig(\"SB_raw_2022-01-17_2022-04-14.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_sb = []  # mape\n",
    "pred_sb = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_sb.append(temp)\n",
    "    pred_sb.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sb[i][0]) \n",
    "    lstm.append(pred_sb[i][1])\n",
    "    hybrid.append(pred_sb[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"Santa Barbara\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[710:710+(train_size+test_size+n_lag)]\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"SB_2022-01-17_2022-04-14.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_sb[i][0]) \n",
    "    mape_lstm.append(mape_sb[i][1])\n",
    "    mape_hybrid.append(mape_sb[i][2])\n",
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_sb[i][0]) \n",
    "    lstm.append(pred_sb[i][1])\n",
    "    hybrid.append(pred_sb[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(max(lstm_std))\n",
    "print(max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riverside 2022-02-16 to 2022-05-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"Riverside\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[740:740+(train_size+test_size+n_lag)]\n",
    "raw_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "#plt.title(\"Training and Testing Set\", fontname = \"Times New Roman\", fontsize='large', fontweight='bold')\n",
    "plt.legend(fontsize = \"large\")\n",
    "# plt.savefig(\"Riv_raw_2022-02-16_2022-05-14.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_riv = []  # mape\n",
    "pred_riv = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_riv.append(temp)\n",
    "    pred_riv.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_riv[i][0]) \n",
    "    lstm.append(pred_riv[i][1])\n",
    "    hybrid.append(pred_riv[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"Riverside\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[740:740+(train_size+test_size+n_lag)]\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"Riv_2022-02-16_2022-05-14.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_riv[i][0]) \n",
    "    mape_lstm.append(mape_riv[i][1])\n",
    "    mape_hybrid.append(mape_riv[i][2])\n",
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_riv[i][0]) \n",
    "    lstm.append(pred_riv[i][1])\n",
    "    hybrid.append(pred_riv[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(max(lstm_std))\n",
    "print(max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6\n",
    "## Jegged Testing: Fresno 2021-02-11 to 2021-05-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = \"Fresno\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[370:370+(train_size+test_size+n_lag)]\n",
    "raw_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "fig.set_size_inches(18, 6, forward=True)\n",
    "plt.plot(raw_value.values, color = 'cornflowerblue', linewidth = 2)\n",
    "# visualize the split of test/train\n",
    "plt.axvline(x = train_size, color = 'orange', linewidth = 2, label = 'train ends')\n",
    "plt.axvline(x = train_size+n_lag, color = 'green', linewidth = 2, label = 'test starts')\n",
    "#plt.title(\"Training and Testing Set\", fontname = \"Times New Roman\", fontsize='large', fontweight='bold')\n",
    "plt.legend(fontsize='large')\n",
    "# plt.savefig(\"Fres_raw_2021-02-11_2021-05-09.png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holders\n",
    "mape_fres = []  # mape\n",
    "pred_fres = []  # predictions\n",
    "# prepare data\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# for AR\n",
    "ARscaler, ARtrain, ARtest = prepare_AR(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "NNscaler, NNtrain, NNtest = prepare_nn(raw_value, diff, train_size, test_size, n_lag, n_seq)\n",
    "for i in range(100):\n",
    "    temp = []\n",
    "    # AR\n",
    "    model_AR = ARIMA(ARtrain, order = (n_lag,0,0), trend = 'c')  # AR, I, MA\n",
    "    model_AR = model_AR.fit()\n",
    "    # make forecasts\n",
    "    forecasts_AR = make_AR(model_AR, raw_value, ARtest, test_size, n_lag, n_seq, diff, ARscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_AR))\n",
    "    # lstm\n",
    "    model_lstm = fit_lstm(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts_lstm = make_lstm(model_lstm, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts_lstm))\n",
    "    # hybrid\n",
    "    model = fit_comb(NNtrain, n_lag, n_seq, n_batch, nb_epoch, n_neurons)\n",
    "    forecasts = make_comb(model, n_batch, raw_value, NNtest, test_size, n_lag, n_seq, diff, NNscaler)\n",
    "    temp.append(evaluate_forecasts(raw_value[-test_size:], forecasts))\n",
    "    # record\n",
    "    mape_fres.append(temp)\n",
    "    pred_fres.append([forecasts_AR, forecasts_lstm, forecasts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_fres[i][0]) \n",
    "    lstm.append(pred_fres[i][1])\n",
    "    hybrid.append(pred_fres[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "\n",
    "# prepare data\n",
    "county = \"Fresno\"\n",
    "s = extract(df1, county)\n",
    "raw_value = s[370:370+(train_size+test_size+n_lag)]\n",
    "truth = raw_value[-test_size:].to_list()\n",
    "# plot\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, axs = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "fig.set_size_inches(16, 10, forward=True)\n",
    "x = np.linspace(0, 18, 18)\n",
    "# AR\n",
    "axs[0].plot(x, AR_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[0].fill_between(x, AR_mean - 2*AR_std, AR_mean + 2*AR_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[0].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[0].set_title(\"AR\", fontsize='large')\n",
    "\n",
    "axs[1].plot(x, lstm_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[1].fill_between(x, lstm_mean - 2*lstm_std, lstm_mean + 2*lstm_std,\n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[1].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[1].set_title(\"LSTM\", fontsize='large')\n",
    "\n",
    "axs[2].plot(x, hybrid_mean, color = 'orange', linewidth = 2, label = \"prediction\")\n",
    "axs[2].fill_between(x, hybrid_mean - 2*hybrid_std, hybrid_mean + 2*hybrid_std, \n",
    "                edgecolor='#CC4F1B', facecolor='#FEEACC', linestyle='dashdot', antialiased=True)\n",
    "axs[2].plot(x, truth, 'b', linewidth = 2, label = \"ground truth\")\n",
    "axs[2].set_title(\"Hybrid\", fontsize='large')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "plt.legend(fontsize= \"large\")\n",
    "#plt.legend(prop={\"family\": \"Times New Roman\"}, fontsize= \"x-large\")\n",
    "fig_name = \"Fres_2021-02-11_2021-05-09.png\"\n",
    "plt.savefig(fig_name, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_AR = []\n",
    "mape_lstm = []\n",
    "mape_hybrid = []\n",
    "for i in range(100):\n",
    "    mape_AR.append(mape_fres[i][0]) \n",
    "    mape_lstm.append(mape_fres[i][1])\n",
    "    mape_hybrid.append(mape_fres[i][2])\n",
    "print(np.mean(mape_AR))\n",
    "print(np.mean(mape_lstm))\n",
    "print(np.mean(mape_hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = []\n",
    "lstm = []\n",
    "hybrid = []\n",
    "for i in range(100):\n",
    "    AR.append(pred_fres[i][0]) \n",
    "    lstm.append(pred_fres[i][1])\n",
    "    hybrid.append(pred_fres[i][2])\n",
    "\n",
    "AR_mean, AR_std = uncertainty(AR)\n",
    "lstm_mean, lstm_std = uncertainty(lstm)\n",
    "hybrid_mean, hybrid_std = uncertainty(hybrid)\n",
    "print(max(lstm_std))\n",
    "print(max(hybrid_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mape_sd).to_csv(\"mape_sd.csv\")\n",
    "pd.DataFrame(pred_sd).to_csv(\"mape_sd.csv\")\n",
    "\n",
    "pd.DataFrame(mape_sf).to_csv(\"mape_sf.csv\")\n",
    "pd.DataFrame(pred_sf).to_csv(\"mape_sf.csv\")\n",
    "\n",
    "pd.DataFrame(mape_la).to_csv(\"mape_la.csv\")\n",
    "pd.DataFrame(pred_la).to_csv(\"mape_la.csv\")\n",
    "\n",
    "pd.DataFrame(mape_sb).to_csv(\"mape_sb.csv\")\n",
    "pd.DataFrame(pred_sb).to_csv(\"mape_sb.csv\")\n",
    "\n",
    "pd.DataFrame(mape_sf_2).to_csv(\"mape_sf_2.csv\")\n",
    "pd.DataFrame(pred_sf_2).to_csv(\"mape_sf_2.csv\")\n",
    "\n",
    "pd.DataFrame(mape_riv).to_csv(\"mape_riv.csv\")\n",
    "pd.DataFrame(pred_riv).to_csv(\"mape_riv.csv\")\n",
    "\n",
    "pd.DataFrame(mape_fres).to_csv(\"mape_fres.csv\")\n",
    "pd.DataFrame(pred_fres).to_csv(\"mape_fres.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
